Module(body=[ImportFrom(module='__future__', names=[alias(name='annotations')], level=0), Import(names=[alias(name='functools')]), ImportFrom(module='concurrent.futures', names=[alias(name='ThreadPoolExecutor')], level=0), ImportFrom(module='typing', names=[alias(name='AbstractSet'), alias(name='Collection'), alias(name='Literal'), alias(name='NoReturn'), alias(name='Optional'), alias(name='Union')], level=0), Import(names=[alias(name='regex')]), ImportFrom(module='tiktoken', names=[alias(name='_tiktoken')], level=0), ClassDef(name='Encoding', bases=[], keywords=[], body=[FunctionDef(name='__init__', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='name', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[arg(arg='pat_str', annotation=Name(id='str', ctx=Load())), arg(arg='mergeable_ranks', annotation=Subscript(value=Name(id='dict', ctx=Load()), slice=Tuple(elts=[Name(id='bytes', ctx=Load()), Name(id='int', ctx=Load())], ctx=Load()), ctx=Load())), arg(arg='special_tokens', annotation=Subscript(value=Name(id='dict', ctx=Load()), slice=Tuple(elts=[Name(id='str', ctx=Load()), Name(id='int', ctx=Load())], ctx=Load()), ctx=Load())), arg(arg='explicit_n_vocab', annotation=Subscript(value=Name(id='Optional', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()))], kw_defaults=[None, None, None, Constant(value=None)], defaults=[]), body=[Expr(value=Constant(value='Creates an Encoding object.\n\n        See openai_public.py for examples of how to construct an Encoding object.\n\n        Args:\n            name: The name of the encoding. It should be clear from the name of the encoding\n                what behaviour to expect, in particular, encodings with different special tokens\n                should have different names.\n            pat_str: A regex pattern string that is used to split the input text.\n            mergeable_ranks: A dictionary mapping mergeable token bytes to their ranks. The ranks\n                must correspond to merge priority.\n            special_tokens: A dictionary mapping special token strings to their token values.\n            explicit_n_vocab: The number of tokens in the vocabulary. If provided, it is checked\n                that the number of mergeable tokens and special tokens is equal to this number.\n        ')), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='name', ctx=Store())], value=Name(id='name', ctx=Load())), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='_pat_str', ctx=Store())], value=Name(id='pat_str', ctx=Load())), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='_mergeable_ranks', ctx=Store())], value=Name(id='mergeable_ranks', ctx=Load())), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='_special_tokens', ctx=Store())], value=Name(id='special_tokens', ctx=Load())), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='max_token_value', ctx=Store())], value=Call(func=Name(id='max', ctx=Load()), args=[Call(func=Name(id='max', ctx=Load()), args=[Call(func=Attribute(value=Name(id='mergeable_ranks', ctx=Load()), attr='values', ctx=Load()), args=[], keywords=[])], keywords=[]), Call(func=Name(id='max', ctx=Load()), args=[Call(func=Attribute(value=Name(id='special_tokens', ctx=Load()), attr='values', ctx=Load()), args=[], keywords=[])], keywords=[keyword(arg='default', value=Constant(value=0))])], keywords=[])), If(test=Name(id='explicit_n_vocab', ctx=Load()), body=[Assert(test=Compare(left=BinOp(left=Call(func=Name(id='len', ctx=Load()), args=[Name(id='mergeable_ranks', ctx=Load())], keywords=[]), op=Add(), right=Call(func=Name(id='len', ctx=Load()), args=[Name(id='special_tokens', ctx=Load())], keywords=[])), ops=[Eq()], comparators=[Name(id='explicit_n_vocab', ctx=Load())])), Assert(test=Compare(left=Attribute(value=Name(id='self', ctx=Load()), attr='max_token_value', ctx=Load()), ops=[Eq()], comparators=[BinOp(left=Name(id='explicit_n_vocab', ctx=Load()), op=Sub(), right=Constant(value=1))]))], orelse=[]), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Store())], value=Call(func=Attribute(value=Name(id='_tiktoken', ctx=Load()), attr='CoreBPE', ctx=Load()), args=[Name(id='mergeable_ranks', ctx=Load()), Name(id='special_tokens', ctx=Load()), Name(id='pat_str', ctx=Load())], keywords=[]))], decorator_list=[]), FunctionDef(name='__repr__', args=arguments(posonlyargs=[], args=[arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Return(value=JoinedStr(values=[Constant(value='<Encoding '), FormattedValue(value=Attribute(value=Name(id='self', ctx=Load()), attr='name', ctx=Load()), conversion=114), Constant(value='>')]))], decorator_list=[], returns=Name(id='str', ctx=Load())), FunctionDef(name='encode_ordinary', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='Encodes a string into tokens, ignoring special tokens.\n\n        This is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\n\n        ```\n        >>> enc.encode_ordinary("hello world")\n        [31373, 995]\n        ')), Try(body=[Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode_ordinary', ctx=Load()), args=[Name(id='text', ctx=Load())], keywords=[]))], handlers=[ExceptHandler(type=Name(id='UnicodeEncodeError', ctx=Load()), body=[Assign(targets=[Name(id='text', ctx=Store())], value=Call(func=Attribute(value=Call(func=Attribute(value=Name(id='text', ctx=Load()), attr='encode', ctx=Load()), args=[Constant(value='utf-16'), Constant(value='surrogatepass')], keywords=[]), attr='decode', ctx=Load()), args=[Constant(value='utf-16'), Constant(value='replace')], keywords=[])), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode_ordinary', ctx=Load()), args=[Name(id='text', ctx=Load())], keywords=[]))])], orelse=[], finalbody=[])], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load())), FunctionDef(name='encode', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[arg(arg='allowed_special', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='Literal', ctx=Load()), slice=Constant(value='all'), ctx=Load()), Subscript(value=Name(id='AbstractSet', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load())), arg(arg='disallowed_special', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='Literal', ctx=Load()), slice=Constant(value='all'), ctx=Load()), Subscript(value=Name(id='Collection', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))], kw_defaults=[Call(func=Name(id='set', ctx=Load()), args=[], keywords=[]), Constant(value='all')], defaults=[]), body=[Expr(value=Constant(value='Encodes a string into tokens.\n\n        Special tokens are artificial tokens used to unlock capabilities from a model,\n        such as fill-in-the-middle. So we want to be careful about accidentally encoding special\n        tokens, since they can be used to trick a model into doing something we don\'t want it to do.\n\n        Hence, by default, encode will raise an error if it encounters text that corresponds\n        to a special token. This can be controlled on a per-token level using the `allowed_special`\n        and `disallowed_special` parameters. In particular:\n        - Setting `disallowed_special` to () will prevent this function from raising errors and\n          cause all text corresponding to special tokens to be encoded as natural text.\n        - Setting `allowed_special` to "all" will cause this function to treat all text\n          corresponding to special tokens to be encoded as special tokens.\n\n        ```\n        >>> enc.encode("hello world")\n        [31373, 995]\n        >>> enc.encode("<|endoftext|>", allowed_special={"<|endoftext|>"})\n        [50256]\n        >>> enc.encode("<|endoftext|>", allowed_special="all")\n        [50256]\n        >>> enc.encode("<|endoftext|>")\n        # Raises ValueError\n        >>> enc.encode("<|endoftext|>", disallowed_special=())\n        [27, 91, 437, 1659, 5239, 91, 29]\n        ```\n        ')), If(test=Compare(left=Name(id='allowed_special', ctx=Load()), ops=[Eq()], comparators=[Constant(value='all')]), body=[Assign(targets=[Name(id='allowed_special', ctx=Store())], value=Attribute(value=Name(id='self', ctx=Load()), attr='special_tokens_set', ctx=Load()))], orelse=[]), If(test=Compare(left=Name(id='disallowed_special', ctx=Load()), ops=[Eq()], comparators=[Constant(value='all')]), body=[Assign(targets=[Name(id='disallowed_special', ctx=Store())], value=BinOp(left=Attribute(value=Name(id='self', ctx=Load()), attr='special_tokens_set', ctx=Load()), op=Sub(), right=Name(id='allowed_special', ctx=Load())))], orelse=[]), If(test=Name(id='disallowed_special', ctx=Load()), body=[If(test=UnaryOp(op=Not(), operand=Call(func=Name(id='isinstance', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load()), Name(id='frozenset', ctx=Load())], keywords=[])), body=[Assign(targets=[Name(id='disallowed_special', ctx=Store())], value=Call(func=Name(id='frozenset', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load())], keywords=[]))], orelse=[]), If(test=NamedExpr(target=Name(id='match', ctx=Store()), value=Call(func=Attribute(value=Call(func=Name(id='_special_token_regex', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load())], keywords=[]), attr='search', ctx=Load()), args=[Name(id='text', ctx=Load())], keywords=[])), body=[Expr(value=Call(func=Name(id='raise_disallowed_special_token', ctx=Load()), args=[Call(func=Attribute(value=Name(id='match', ctx=Load()), attr='group', ctx=Load()), args=[], keywords=[])], keywords=[]))], orelse=[])], orelse=[]), Try(body=[Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode', ctx=Load()), args=[Name(id='text', ctx=Load()), Name(id='allowed_special', ctx=Load())], keywords=[]))], handlers=[ExceptHandler(type=Name(id='UnicodeEncodeError', ctx=Load()), body=[Assign(targets=[Name(id='text', ctx=Store())], value=Call(func=Attribute(value=Call(func=Attribute(value=Name(id='text', ctx=Load()), attr='encode', ctx=Load()), args=[Constant(value='utf-16'), Constant(value='surrogatepass')], keywords=[]), attr='decode', ctx=Load()), args=[Constant(value='utf-16'), Constant(value='replace')], keywords=[])), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode', ctx=Load()), args=[Name(id='text', ctx=Load()), Name(id='allowed_special', ctx=Load())], keywords=[]))])], orelse=[], finalbody=[])], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load())), FunctionDef(name='encode_ordinary_batch', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load()))], kwonlyargs=[arg(arg='num_threads', annotation=Name(id='int', ctx=Load()))], kw_defaults=[Constant(value=8)], defaults=[]), body=[Expr(value=Constant(value='Encodes a list of strings into tokens, in parallel, ignoring special tokens.\n\n        This is equivalent to `encode_batch(text, disallowed_special=())` (but slightly faster).\n\n        ```\n        >>> enc.encode_ordinary_batch(["hello world", "goodbye world"])\n        [[31373, 995], [11274, 16390, 995]]\n        ```\n        ')), Assign(targets=[Name(id='encoder', ctx=Store())], value=Call(func=Attribute(value=Name(id='functools', ctx=Load()), attr='partial', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='encode_ordinary', ctx=Load())], keywords=[])), With(items=[withitem(context_expr=Call(func=Name(id='ThreadPoolExecutor', ctx=Load()), args=[Name(id='num_threads', ctx=Load())], keywords=[]), optional_vars=Name(id='e', ctx=Store()))], body=[Return(value=Call(func=Name(id='list', ctx=Load()), args=[Call(func=Attribute(value=Name(id='e', ctx=Load()), attr='map', ctx=Load()), args=[Name(id='encoder', ctx=Load()), Name(id='text', ctx=Load())], keywords=[])], keywords=[]))])], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()), ctx=Load())), FunctionDef(name='encode_batch', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load()))], kwonlyargs=[arg(arg='num_threads', annotation=Name(id='int', ctx=Load())), arg(arg='allowed_special', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='Literal', ctx=Load()), slice=Constant(value='all'), ctx=Load()), Subscript(value=Name(id='AbstractSet', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load())), arg(arg='disallowed_special', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='Literal', ctx=Load()), slice=Constant(value='all'), ctx=Load()), Subscript(value=Name(id='Collection', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))], kw_defaults=[Constant(value=8), Call(func=Name(id='set', ctx=Load()), args=[], keywords=[]), Constant(value='all')], defaults=[]), body=[Expr(value=Constant(value='Encodes a list of strings into tokens, in parallel.\n\n        See `encode` for more details on `allowed_special` and `disallowed_special`.\n\n        ```\n        >>> enc.encode_batch(["hello world", "goodbye world"])\n        [[31373, 995], [11274, 16390, 995]]\n        ```\n        ')), If(test=Compare(left=Name(id='allowed_special', ctx=Load()), ops=[Eq()], comparators=[Constant(value='all')]), body=[Assign(targets=[Name(id='allowed_special', ctx=Store())], value=Attribute(value=Name(id='self', ctx=Load()), attr='special_tokens_set', ctx=Load()))], orelse=[]), If(test=Compare(left=Name(id='disallowed_special', ctx=Load()), ops=[Eq()], comparators=[Constant(value='all')]), body=[Assign(targets=[Name(id='disallowed_special', ctx=Store())], value=BinOp(left=Attribute(value=Name(id='self', ctx=Load()), attr='special_tokens_set', ctx=Load()), op=Sub(), right=Name(id='allowed_special', ctx=Load())))], orelse=[]), If(test=UnaryOp(op=Not(), operand=Call(func=Name(id='isinstance', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load()), Name(id='frozenset', ctx=Load())], keywords=[])), body=[Assign(targets=[Name(id='disallowed_special', ctx=Store())], value=Call(func=Name(id='frozenset', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load())], keywords=[]))], orelse=[]), Assign(targets=[Name(id='encoder', ctx=Store())], value=Call(func=Attribute(value=Name(id='functools', ctx=Load()), attr='partial', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='encode', ctx=Load())], keywords=[keyword(arg='allowed_special', value=Name(id='allowed_special', ctx=Load())), keyword(arg='disallowed_special', value=Name(id='disallowed_special', ctx=Load()))])), With(items=[withitem(context_expr=Call(func=Name(id='ThreadPoolExecutor', ctx=Load()), args=[Name(id='num_threads', ctx=Load())], keywords=[]), optional_vars=Name(id='e', ctx=Store()))], body=[Return(value=Call(func=Name(id='list', ctx=Load()), args=[Call(func=Attribute(value=Name(id='e', ctx=Load()), attr='map', ctx=Load()), args=[Name(id='encoder', ctx=Load()), Name(id='text', ctx=Load())], keywords=[])], keywords=[]))])], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()), ctx=Load())), FunctionDef(name='encode_with_unstable', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[arg(arg='allowed_special', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='Literal', ctx=Load()), slice=Constant(value='all'), ctx=Load()), Subscript(value=Name(id='AbstractSet', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load())), arg(arg='disallowed_special', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='Literal', ctx=Load()), slice=Constant(value='all'), ctx=Load()), Subscript(value=Name(id='Collection', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))], kw_defaults=[Call(func=Name(id='set', ctx=Load()), args=[], keywords=[]), Constant(value='all')], defaults=[]), body=[Expr(value=Constant(value='Encodes a string into stable tokens and possible completion sequences.\n\n        Note that the stable tokens will only represent a substring of `text`.\n\n        See `encode` for more details on `allowed_special` and `disallowed_special`.\n\n        This API should itself be considered unstable.\n\n        ```\n        >>> enc.encode_with_unstable("hello fanta")\n        ([31373], [(277, 4910), (5113, 265), ..., (8842,)])\n\n        >>> text = "..."\n        >>> stable_tokens, completions = enc.encode_with_unstable(text)\n        >>> assert text.encode().startswith(enc.decode_bytes(stable_tokens))\n        >>> assert all(enc.decode_bytes(stable_tokens + seq).startswith(text.encode()) for seq in completions)\n        ```\n        ')), If(test=Compare(left=Name(id='allowed_special', ctx=Load()), ops=[Eq()], comparators=[Constant(value='all')]), body=[Assign(targets=[Name(id='allowed_special', ctx=Store())], value=Attribute(value=Name(id='self', ctx=Load()), attr='special_tokens_set', ctx=Load()))], orelse=[]), If(test=Compare(left=Name(id='disallowed_special', ctx=Load()), ops=[Eq()], comparators=[Constant(value='all')]), body=[Assign(targets=[Name(id='disallowed_special', ctx=Store())], value=BinOp(left=Attribute(value=Name(id='self', ctx=Load()), attr='special_tokens_set', ctx=Load()), op=Sub(), right=Name(id='allowed_special', ctx=Load())))], orelse=[]), If(test=Name(id='disallowed_special', ctx=Load()), body=[If(test=UnaryOp(op=Not(), operand=Call(func=Name(id='isinstance', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load()), Name(id='frozenset', ctx=Load())], keywords=[])), body=[Assign(targets=[Name(id='disallowed_special', ctx=Store())], value=Call(func=Name(id='frozenset', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load())], keywords=[]))], orelse=[]), If(test=NamedExpr(target=Name(id='match', ctx=Store()), value=Call(func=Attribute(value=Call(func=Name(id='_special_token_regex', ctx=Load()), args=[Name(id='disallowed_special', ctx=Load())], keywords=[]), attr='search', ctx=Load()), args=[Name(id='text', ctx=Load())], keywords=[])), body=[Expr(value=Call(func=Name(id='raise_disallowed_special_token', ctx=Load()), args=[Call(func=Attribute(value=Name(id='match', ctx=Load()), attr='group', ctx=Load()), args=[], keywords=[])], keywords=[]))], orelse=[])], orelse=[]), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode_with_unstable', ctx=Load()), args=[Name(id='text', ctx=Load()), Name(id='allowed_special', ctx=Load())], keywords=[]))], decorator_list=[], returns=Subscript(value=Name(id='tuple', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()), Subscript(value=Name(id='list', ctx=Load()), slice=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load())), FunctionDef(name='encode_single_token', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text_or_bytes', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Name(id='str', ctx=Load()), Name(id='bytes', ctx=Load())], ctx=Load()), ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='Encodes text corresponding to a single token to its token value.\n\n        NOTE: this will encode all special tokens.\n\n        Raises `KeyError` if the token is not in the vocabulary.\n\n        ```\n        >>> enc.encode_single_token("hello")\n        31373\n        ```\n        ')), If(test=Call(func=Name(id='isinstance', ctx=Load()), args=[Name(id='text_or_bytes', ctx=Load()), Name(id='str', ctx=Load())], keywords=[]), body=[Assign(targets=[Name(id='text_or_bytes', ctx=Store())], value=Call(func=Attribute(value=Name(id='text_or_bytes', ctx=Load()), attr='encode', ctx=Load()), args=[Constant(value='utf-8')], keywords=[]))], orelse=[]), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode_single_token', ctx=Load()), args=[Name(id='text_or_bytes', ctx=Load())], keywords=[]))], decorator_list=[], returns=Name(id='int', ctx=Load())), FunctionDef(name='decode_bytes', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='tokens', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value="Decodes a list of tokens into bytes.\n\n        ```\n        >>> enc.decode_bytes([31373, 995])\n        b'hello world'\n        ```\n        ")), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='decode_bytes', ctx=Load()), args=[Name(id='tokens', ctx=Load())], keywords=[]))], decorator_list=[], returns=Name(id='bytes', ctx=Load())), FunctionDef(name='decode', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='tokens', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load())), arg(arg='errors', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[Constant(value='replace')]), body=[Expr(value=Constant(value="Decodes a list of tokens into a string.\n\n        WARNING: the default behaviour of this function is lossy, since decoded bytes are not\n        guaranteed to be valid UTF-8. You can control this behaviour using the `errors` parameter,\n        for instance, setting `errors=strict`.\n\n        ```\n        >>> enc.decode([31373, 995])\n        'hello world'\n        ```\n        ")), Return(value=Call(func=Attribute(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='decode_bytes', ctx=Load()), args=[Name(id='tokens', ctx=Load())], keywords=[]), attr='decode', ctx=Load()), args=[Constant(value='utf-8')], keywords=[keyword(arg='errors', value=Name(id='errors', ctx=Load()))]))], decorator_list=[], returns=Name(id='str', ctx=Load())), FunctionDef(name='decode_single_token_bytes', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='token', annotation=Name(id='int', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value="Decodes a token into bytes.\n\n        NOTE: this will decode all special tokens.\n\n        Raises `KeyError` if the token is not in the vocabulary.\n\n        ```\n        >>> enc.decode_single_token_bytes(31373)\n        b'hello'\n        ```\n        ")), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='decode_single_token_bytes', ctx=Load()), args=[Name(id='token', ctx=Load())], keywords=[]))], decorator_list=[], returns=Name(id='bytes', ctx=Load())), FunctionDef(name='decode_tokens_bytes', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='tokens', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value="Decodes a list of tokens into a list of bytes.\n\n        Useful for visualising tokenisation.\n        >>> enc.decode_tokens_bytes([31373, 995])\n        [b'hello', b' world']\n        ")), Return(value=ListComp(elt=Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='decode_single_token_bytes', ctx=Load()), args=[Name(id='token', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id='token', ctx=Store()), iter=Name(id='tokens', ctx=Load()), ifs=[], is_async=0)]))], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='bytes', ctx=Load()), ctx=Load())), FunctionDef(name='decode_with_offsets', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='tokens', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value="Decodes a list of tokens into a string and a list of offsets.\n\n        Each offset is the index into text corresponding to the start of each token.\n        If UTF-8 character boundaries do not line up with token boundaries, the offset is the index\n        of the first character that contains bytes from the token.\n\n        This will currently raise if given tokens that decode to invalid UTF-8; this behaviour may\n        change in the future to be more permissive.\n\n        >>> enc.decode_with_offsets([31373, 995])\n        ('hello world', [0, 5])\n        ")), Assign(targets=[Name(id='token_bytes', ctx=Store())], value=Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='decode_tokens_bytes', ctx=Load()), args=[Name(id='tokens', ctx=Load())], keywords=[])), Assign(targets=[Name(id='text_len', ctx=Store())], value=Constant(value=0)), Assign(targets=[Name(id='offsets', ctx=Store())], value=List(elts=[], ctx=Load())), For(target=Name(id='token', ctx=Store()), iter=Name(id='token_bytes', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id='offsets', ctx=Load()), attr='append', ctx=Load()), args=[Call(func=Name(id='max', ctx=Load()), args=[Constant(value=0), BinOp(left=Name(id='text_len', ctx=Load()), op=Sub(), right=Compare(left=Constant(value=128), ops=[LtE(), Lt()], comparators=[Subscript(value=Name(id='token', ctx=Load()), slice=Constant(value=0), ctx=Load()), Constant(value=192)]))], keywords=[])], keywords=[])), AugAssign(target=Name(id='text_len', ctx=Store()), op=Add(), value=Call(func=Name(id='sum', ctx=Load()), args=[GeneratorExp(elt=Constant(value=1), generators=[comprehension(target=Name(id='c', ctx=Store()), iter=Name(id='token', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Compare(left=Constant(value=128), ops=[LtE(), Lt()], comparators=[Name(id='c', ctx=Load()), Constant(value=192)]))], is_async=0)])], keywords=[]))], orelse=[]), Assign(targets=[Name(id='text', ctx=Store())], value=Call(func=Attribute(value=Call(func=Attribute(value=Constant(value=b''), attr='join', ctx=Load()), args=[Name(id='token_bytes', ctx=Load())], keywords=[]), attr='decode', ctx=Load()), args=[Constant(value='utf-8')], keywords=[keyword(arg='errors', value=Constant(value='strict'))])), Return(value=Tuple(elts=[Name(id='text', ctx=Load()), Name(id='offsets', ctx=Load())], ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id='tuple', ctx=Load()), slice=Tuple(elts=[Name(id='str', ctx=Load()), Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load())), FunctionDef(name='decode_batch', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='batch', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()), ctx=Load()))], kwonlyargs=[arg(arg='errors', annotation=Name(id='str', ctx=Load())), arg(arg='num_threads', annotation=Name(id='int', ctx=Load()))], kw_defaults=[Constant(value='replace'), Constant(value=8)], defaults=[]), body=[Expr(value=Constant(value='Decodes a batch (list of lists of tokens) into a list of strings.')), Assign(targets=[Name(id='decoder', ctx=Store())], value=Call(func=Attribute(value=Name(id='functools', ctx=Load()), attr='partial', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='decode', ctx=Load())], keywords=[keyword(arg='errors', value=Name(id='errors', ctx=Load()))])), With(items=[withitem(context_expr=Call(func=Name(id='ThreadPoolExecutor', ctx=Load()), args=[Name(id='num_threads', ctx=Load())], keywords=[]), optional_vars=Name(id='e', ctx=Store()))], body=[Return(value=Call(func=Name(id='list', ctx=Load()), args=[Call(func=Attribute(value=Name(id='e', ctx=Load()), attr='map', ctx=Load()), args=[Name(id='decoder', ctx=Load()), Name(id='batch', ctx=Load())], keywords=[])], keywords=[]))])], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())), FunctionDef(name='decode_bytes_batch', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='batch', annotation=Subscript(value=Name(id='list', ctx=Load()), slice=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()), ctx=Load()))], kwonlyargs=[arg(arg='num_threads', annotation=Name(id='int', ctx=Load()))], kw_defaults=[Constant(value=8)], defaults=[]), body=[Expr(value=Constant(value='Decodes a batch (list of lists of tokens) into a list of bytes.')), With(items=[withitem(context_expr=Call(func=Name(id='ThreadPoolExecutor', ctx=Load()), args=[Name(id='num_threads', ctx=Load())], keywords=[]), optional_vars=Name(id='e', ctx=Store()))], body=[Return(value=Call(func=Name(id='list', ctx=Load()), args=[Call(func=Attribute(value=Name(id='e', ctx=Load()), attr='map', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='decode_bytes', ctx=Load()), Name(id='batch', ctx=Load())], keywords=[])], keywords=[]))])], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='bytes', ctx=Load()), ctx=Load())), FunctionDef(name='token_byte_values', args=arguments(posonlyargs=[], args=[arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='Returns the list of all token byte values.')), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='token_byte_values', ctx=Load()), args=[], keywords=[]))], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='bytes', ctx=Load()), ctx=Load())), FunctionDef(name='eot_token', args=arguments(posonlyargs=[], args=[arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Return(value=Subscript(value=Attribute(value=Name(id='self', ctx=Load()), attr='_special_tokens', ctx=Load()), slice=Constant(value='<|endoftext|>'), ctx=Load()))], decorator_list=[Name(id='property', ctx=Load())], returns=Name(id='int', ctx=Load())), FunctionDef(name='special_tokens_set', args=arguments(posonlyargs=[], args=[arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Return(value=Call(func=Name(id='set', ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_special_tokens', ctx=Load()), attr='keys', ctx=Load()), args=[], keywords=[])], keywords=[]))], decorator_list=[Attribute(value=Name(id='functools', ctx=Load()), attr='cached_property', ctx=Load())], returns=Subscript(value=Name(id='set', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load())), FunctionDef(name='n_vocab', args=arguments(posonlyargs=[], args=[arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='For backwards compatibility. Prefer to use `enc.max_token_value + 1`.')), Return(value=BinOp(left=Attribute(value=Name(id='self', ctx=Load()), attr='max_token_value', ctx=Load()), op=Add(), right=Constant(value=1)))], decorator_list=[Name(id='property', ctx=Load())], returns=Name(id='int', ctx=Load())), FunctionDef(name='_encode_single_piece', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text_or_bytes', annotation=Subscript(value=Name(id='Union', ctx=Load()), slice=Tuple(elts=[Name(id='str', ctx=Load()), Name(id='bytes', ctx=Load())], ctx=Load()), ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='Encodes text corresponding to bytes without a regex split.\n\n        NOTE: this will not encode any special tokens.\n\n        ```\n        >>> enc.encode_single_piece("helloqqqq")\n        [31373, 38227, 38227]\n        ```\n        ')), If(test=Call(func=Name(id='isinstance', ctx=Load()), args=[Name(id='text_or_bytes', ctx=Load()), Name(id='str', ctx=Load())], keywords=[]), body=[Assign(targets=[Name(id='text_or_bytes', ctx=Store())], value=Call(func=Attribute(value=Name(id='text_or_bytes', ctx=Load()), attr='encode', ctx=Load()), args=[Constant(value='utf-8')], keywords=[]))], orelse=[]), Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode_single_piece', ctx=Load()), args=[Name(id='text_or_bytes', ctx=Load())], keywords=[]))], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load())), FunctionDef(name='_encode_only_native_bpe', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value='Encodes a string into tokens, but do regex splitting in Python.')), Assign(targets=[Name(id='_unused_pat', ctx=Store())], value=Call(func=Attribute(value=Name(id='regex', ctx=Load()), attr='compile', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='_pat_str', ctx=Load())], keywords=[])), Assign(targets=[Name(id='ret', ctx=Store())], value=List(elts=[], ctx=Load())), For(target=Name(id='piece', ctx=Store()), iter=Call(func=Attribute(value=Name(id='regex', ctx=Load()), attr='findall', ctx=Load()), args=[Name(id='_unused_pat', ctx=Load()), Name(id='text', ctx=Load())], keywords=[]), body=[Expr(value=Call(func=Attribute(value=Name(id='ret', ctx=Load()), attr='extend', ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='encode_single_piece', ctx=Load()), args=[Name(id='piece', ctx=Load())], keywords=[])], keywords=[]))], orelse=[]), Return(value=Name(id='ret', ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load())), FunctionDef(name='_encode_bytes', args=arguments(posonlyargs=[], args=[arg(arg='self'), arg(arg='text', annotation=Name(id='bytes', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Return(value=Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='_core_bpe', ctx=Load()), attr='_encode_bytes', ctx=Load()), args=[Name(id='text', ctx=Load())], keywords=[]))], decorator_list=[], returns=Subscript(value=Name(id='list', ctx=Load()), slice=Name(id='int', ctx=Load()), ctx=Load()))], decorator_list=[]), FunctionDef(name='_special_token_regex', args=arguments(posonlyargs=[], args=[arg(arg='tokens', annotation=Subscript(value=Name(id='frozenset', ctx=Load()), slice=Name(id='str', ctx=Load()), ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='inner', ctx=Store())], value=Call(func=Attribute(value=Constant(value='|'), attr='join', ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Name(id='regex', ctx=Load()), attr='escape', ctx=Load()), args=[Name(id='token', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id='token', ctx=Store()), iter=Name(id='tokens', ctx=Load()), ifs=[], is_async=0)])], keywords=[])), Return(value=Call(func=Attribute(value=Name(id='regex', ctx=Load()), attr='compile', ctx=Load()), args=[JoinedStr(values=[Constant(value='('), FormattedValue(value=Name(id='inner', ctx=Load()), conversion=-1), Constant(value=')')])], keywords=[]))], decorator_list=[Call(func=Attribute(value=Name(id='functools', ctx=Load()), attr='lru_cache', ctx=Load()), args=[], keywords=[keyword(arg='maxsize', value=Constant(value=128))])], returns=Constant(value='regex.Pattern[str]')), FunctionDef(name='raise_disallowed_special_token', args=arguments(posonlyargs=[], args=[arg(arg='token', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Raise(exc=Call(func=Name(id='ValueError', ctx=Load()), args=[JoinedStr(values=[Constant(value='Encountered text corresponding to disallowed special token '), FormattedValue(value=Name(id='token', ctx=Load()), conversion=114), Constant(value='.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'), FormattedValue(value=Name(id='token', ctx=Load()), conversion=114), Constant(value=', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'), FormattedValue(value=Name(id='token', ctx=Load()), conversion=114), Constant(value='})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n')])], keywords=[]))], decorator_list=[], returns=Name(id='NoReturn', ctx=Load()))], type_ignores=[])